{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob, Word\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "\n",
    "#magic\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data from reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Authentication (removed)\n",
    "import praw\n",
    "\n",
    "rdt = praw.Reddit(client_id = \"xxxx\", client_secret = \"xxx\", password = \"xxx\",\n",
    "                  username = \"xxx\", user_agent = \"xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grab data from reddit\n",
    "posts = list()\n",
    "source = list()\n",
    "text = list()\n",
    "author = list()\n",
    "crossposts = list()\n",
    "time = list()\n",
    "sub_r = rdt.subreddit(\"conspiracy\")\n",
    "for sub in sub_r.submissions(1514764800, 1517875200):\n",
    "    if sub is not None:\n",
    "        posts.append(sub.title)\n",
    "        source.append(sub)\n",
    "        text.append(sub.selftext)\n",
    "        author.append(sub.author)\n",
    "        crossposts.append(sub.num_crossposts)\n",
    "        time.append(sub.created)\n",
    "    else:\n",
    "        text.append(None)\n",
    "        crossposts.append(None)\n",
    "        time.append(None)\n",
    "        \n",
    "conspiracy_reddit = pd.DataFrame(np.column_stack((time, posts, text, author, source)), \n",
    "                                 columns = [\"time\", \"posts\", \"text\", \"author\", \"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set the index\n",
    "conspiracy_reddit[\"time\"] = (pd.to_datetime(conspiracy_reddit[\"time\"],unit=\"ms\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#send to csv\n",
    "conspiracy_reddit.to_csv(\"redditcons\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grab csv as dataframe\n",
    "df = pd.read_csv(\"redditconspiracy/redditcons\", sep = \"\\t\", index_col = \"time\")\n",
    "df = df.drop(\"Unnamed: 0\", axis = 1)\n",
    "df[\"text\"] = df.text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean up the posts text\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stop = stopwords.words(\"english\")\n",
    "stop2 = [\"get\", \"r\", \"like\", \"us\"]\n",
    "def stopwords(x):\n",
    "    x = re.sub(\"[^a-z\\s]\", \" \", x.lower())\n",
    "    x = [w for w in x.split() if w not in set(stop) and w not in stop2]\n",
    "    return \" \".join(x)\n",
    "\n",
    "df[\"posts_cleaned\"] = df[\"posts\"].apply(stopwords)\n",
    "df[\"text_cleaned\"] = df[\"text\"].apply(stopwords)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenize + tag the posts\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from textblob import TextBlob\n",
    "\n",
    "df[\"posts_stokenized\"] = df[\"posts\"].apply(sent_tokenize)\n",
    "df[\"text_stokenized\"] = df[\"text\"].apply(sent_tokenize)\n",
    "df[\"posts_wtokenized\"] = df[\"posts\"].apply(word_tokenize)\n",
    "df[\"text_wtokenized\"] =  df[\"text\"].apply(word_tokenize)\n",
    "df[\"posts_tagged\"] = pos_tag_sents(df[\"posts\"].apply(word_tokenize).tolist())\n",
    "df[\"text_tagged\"] =  pos_tag_sents(df[\"text\"].apply(word_tokenize).tolist()) \n",
    "df[\"posts_nouns\"] = df[\"posts\"].astype(str).apply(lambda x: TextBlob(x).noun_phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import BigramCollocationFinder\n",
    "\n",
    "#most common words from posts: \n",
    "words = pd.Series(\"\".join(df[\"posts_cleaned\"]).lower().split()).value_counts()[:1000]\n",
    "conspiracy_words = []\n",
    "for word in words.keys():\n",
    "    conspiracy_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write that list to a file\n",
    "with open(\"conspiracy1000words.txt\", \"w\") as f:\n",
    "    for item in conspiracy_words:\n",
    "        f.write(item + \"\\n\")\n",
    "    \n",
    "f.close()\n",
    "print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the bigrams\n",
    "vectorizer = CountVectorizer(ngram_range = (2, 2), analyzer = \"word\")\n",
    "sparse_matrix = vectorizer.fit_transform(df[\"posts_cleaned\"])\n",
    "frequencies = sum(sparse_matrix).toarray()[0]\n",
    "dfConspiracy = pd.DataFrame(frequencies, index = vectorizer.get_feature_names(), columns = [\"frequency\"])\n",
    "dfConspiracy.sort_values(by=[\"frequency\"], ascending = False, inplace = True)\n",
    "dfConspiracy.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cable News Chyrons\n",
    "Files from Nov. 2017 - Feb. 2018 of every 15 minutes taken from the Television Archives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bring in the directory\n",
    "import glob\n",
    "path = r\"C:/Users/bpeake/dropbox/1_DataSci/data/Cheyron2018\"\n",
    "allFiles = glob.glob(path + \"/*.tsv\")\n",
    "list_ = []\n",
    "for file in allFiles:\n",
    "    chyron = pd.read_csv(file, index_col = None, header = None, sep = \"\\t\")\n",
    "    list_.append(chyron)\n",
    "\n",
    "df = pd.concat(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns = [\"timestamp\", \"channel\", \"retweets\", \"show\", \"text\"]\n",
    "df.set_index(\"timestamp\", inplace = True)\n",
    "df[\"text\"] = df[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create conspiracy corpus from r/conspiracy\n",
    "conspiracies = []\n",
    "\n",
    "#These are the files I want to read in\n",
    "files = [\"redditconspiracy/conspiracy1000words.txt\", \n",
    "         \"redditconspiracy/conspiracy100bigrams.txt\",\n",
    "         \"redditconspiracy/conspiracy100trigrams.txt\"]\n",
    "\n",
    "#populate the list with lists from each file\n",
    "for file in files:\n",
    "    with open(file, \"r\") as f:\n",
    "         lines = f.read().splitlines()\n",
    "         conspiracies.append(lines)\n",
    "\n",
    "#create\n",
    "conspiracy_list = []\n",
    "conspiracy_items = [conspiracies[1], conspiracies[2]]\n",
    "for thing in conspiracy_items:\n",
    "    for stuff in thing:\n",
    "        conspiracy_list.append(stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Measure conspiracy against the dataframe, export to csv\n",
    "conspiracy = \"|\".join(conspiracy_list)\n",
    "df[\"conspiracy_talk\"] = df[\"text\"].str.contains(conspiracy)\n",
    "df.to_csv(\"conspiracy_news\", sep = \"\\t\", encoding = \"utf-8\")\n",
    "\n",
    "#map the boolean to numbers\n",
    "df[\"conspiracy_talk\"] = df[\"conspiracy_talk\"].map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create channel-specific dataframes\n",
    "fox = df[df.channel == \"FOXNEWSW\"]\n",
    "bbc = df[df.channel == \"BBCNEWS\"]\n",
    "cnn = df[df.channel == \"CNNW\"]\n",
    "msnbc = df[df.channel == \"MSNBCW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the news stats\n",
    "print(\"FOX\")\n",
    "print(fox.conspiracy_talk.describe())\n",
    "print(\" \")\n",
    "print(\"CNN\")\n",
    "print(cnn.conspiracy_talk.describe())\n",
    "print(\" \")\n",
    "print(\"MSNBC\")\n",
    "print(msnbc.conspiracy_talk.describe())\n",
    "print(\" \")\n",
    "print(\"BBC\")\n",
    "print(bbc.conspiracy_talk.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dependencies\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from textblob import TextBlob\n",
    "\n",
    "#create dataframe for news with conspiracy\n",
    "news_w_conspiracy = df[df.conspiracy_talk == True]\n",
    "\n",
    "#tagging conspiracy language\n",
    "news_w_conspiracy[\"text_stokenized\"] = news_w_conspiracy[\"text\"].apply(sent_tokenize)\n",
    "news_w_conspiracy[\"text_wtokenized\"] =  news_w_conspiracy[\"text\"].apply(word_tokenize)\n",
    "news_w_conspiracy[\"text_tagged\"] =  pos_tag_sents(news_w_conspiracy[\"text\"].apply(word_tokenize).tolist()) \n",
    "news_w_conspiracy[\"posts_nouns\"] = news_w_conspiracy[\"text\"].astype(str).apply(lambda x: TextBlob(x).noun_phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dependencies\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# build stopwords function\n",
    "stop = stopwords.words(\"english\")\n",
    "stop2 = [\"get\", \"r\", \"like\", \"us\"]\n",
    "def stopwords(x):\n",
    "    x = re.sub(\"[^a-z\\s]\", \" \", x.lower())\n",
    "    x = [w for w in x.split() if w not in set(stop) and w not in stop2]\n",
    "    return \" \".join(x)\n",
    "\n",
    "#clean text column\n",
    "news_w_conspiracy[\"text_cleaned\"] = news_w_conspiracy[\"text\"].apply(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build conspiracy news bigrams/trigrams that simulate headlines\n",
    "#dependencies\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import BigramCollocationFinder\n",
    "\n",
    "#bigrams\n",
    "vectorizer = CountVectorizer(ngram_range = (2, 2), analyzer = \"word\")\n",
    "sparse_matrix = vectorizer.fit_transform(news_w_conspiracy[\"text_cleaned\"])\n",
    "frequencies = sum(sparse_matrix).toarray()[0]\n",
    "dfConspiracy = pd.DataFrame(frequencies, index = vectorizer.get_feature_names(), columns = [\"frequency\"])\n",
    "dfConspiracy.sort_values(by=[\"frequency\"], ascending = False, inplace = True)\n",
    "\n",
    "with open(\"cons_news100bigrams.txt\", \"w\") as f:\n",
    "    for item in dfConspiracy.index:\n",
    "        f.write(item + \"\\n\")\n",
    "\n",
    "f.close()\n",
    "\n",
    "#trigrams\n",
    "vectorizer = CountVectorizer(ngram_range = (3, 3), analyzer = \"word\")\n",
    "sparse_matrix = vectorizer.fit_transform(news_w_conspiracy[\"text_cleaned\"])\n",
    "frequencies = sum(sparse_matrix).toarray()[0]\n",
    "dfConspiracy3 = pd.DataFrame(frequencies, index = vectorizer.get_feature_names(), columns = [\"frequency\"])\n",
    "dfConspiracy3.sort_values(by=[\"frequency\"], ascending = False, inplace = True)\n",
    "\n",
    "with open(\"cons_news100trigrams.txt\", \"w\") as f:\n",
    "    for item in dfConspiracy3.index:\n",
    "        f.write(item + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter analysis\n",
    "Taken from the past 3 months of Donald Trump's twitter. Taken from previously built dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"TrumpTweets112017.txt\", sep = \",\", encoding = \"utf-8\", index_col = \"created_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the conspiracy news dictionary\n",
    "conspiracy_news = []\n",
    "files = [\"cons_news100bigrams.txt\",\n",
    "         \"cons_news100trigrams.txt\"]\n",
    "for file in files:\n",
    "    with open(file, \"r\") as f: \n",
    "        lines = f.read().splitlines()\n",
    "        conspiracy_news.append(lines)\n",
    "\n",
    "conspiracy_news_items = []\n",
    "conspiracy_news_list = [conspiracy_news[0], conspiracy_news[1]]\n",
    "for thing in conspiracy_news_list:\n",
    "    for stuff in thing: \n",
    "        conspiracy_news_items.append(stuff)\n",
    "        \n",
    "#make the list comprehendable to pandas .contain()\n",
    "conspiracy_news = \"|\".join(conspiracy_news_items)\n",
    "\n",
    "#create the dataframe and export it as a csv\n",
    "df[\"conspiracy_news\"] = df[\"text\"].str.contains(conspiracy_news)df.drop(\"id_str\", axis = 1, inplace = True)\n",
    "df.drop(\"id_str\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create conspiracy dictionary\n",
    "conspiracies = []\n",
    "\n",
    "#These are the files I want to read in\n",
    "files = [\"redditconspiracy/conspiracy1000words.txt\", \n",
    "         \"redditconspiracy/conspiracy100bigrams.txt\",\n",
    "         \"redditconspiracy/conspiracy100trigrams.txt\"]\n",
    "\n",
    "#populate the list with lists from each file\n",
    "for file in files:\n",
    "    with open(file, \"r\") as f:\n",
    "         lines = f.read().splitlines()\n",
    "         conspiracies.append(lines)\n",
    "\n",
    "#create\n",
    "conspiracy_list = []\n",
    "conspiracy_items = [conspiracies[1], conspiracies[2]]\n",
    "for thing in conspiracy_items:\n",
    "    for stuff in thing:\n",
    "        conspiracy_list.append(stuff)\n",
    "\n",
    "conspiracy = \"|\".join(conspiracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a column about conspiracy talk, send to csv\n",
    "df[\"conspiracy_talk\"] = df[\"text\"].str.contains(conspiracy)\n",
    "df.to_csv(\"trump_cons_twitter.tsv\", sep = \"\\t\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the various dataframes\n",
    "news = pd.read_csv(\"conspiracy_news.tsv\", sep = \"\\t\", \n",
    "                   index_col = \"timestamp\", encoding = \"utf-8\")\n",
    "trump = pd.read_csv(\"trump_cons_twitter.tsv\", sep = \"\\t\", index_col = \"created_at\", encoding = \"utf-8\")\n",
    "conspiracy = pd.read_csv(\"redditconspiracy/RedditConspiracy2018.csv\", \n",
    "                         sep = \"\\t\", index_col = \"time\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#relationship between favorites and retweets by conspiracy news\n",
    "trump_consp_news = trump[trump.conspiracy_reference == True]\n",
    "trump_consp_news.drop(\"text\", axis = 1, inplace = True)\n",
    "trump_no_consp_news = trump[trump.conspiracy_reference == False]\n",
    "trump_consp_news.plot(kind = \"scatter\", x  = \"retweet_count\", y = \"favorite_count\", alpha = 0.3)\n",
    "trump_no_consp_news.plot(kind = \"scatter\", x = \"retweet_count\", y = \"favorite_count\", alpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#relationship between favorites and retweets by conspiracy talk\n",
    "trump_consp_talk = trump[trump.conspiracy_talk == True]\n",
    "trump_no_consp_talk = trump[trump.conspiracy_talk == False]\n",
    "trump_consp_talk.plot(kind = \"scatter\", x  = \"retweet_count\", y = \"favorite_count\", alpha = 0.3)\n",
    "trump_no_consp_talk.plot(kind = \"scatter\", x = \"retweet_count\", y = \"favorite_count\", alpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#boxplot of retweet/favorite count by mention of conspiracy\n",
    "trump.boxplot(column = \"retweet_count\", by = \"conspiracy_reference\")\n",
    "trump.boxplot(column = \"favorite_count\", by = \"conspiracy_talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#distribution of retweets by conspiracy talk\n",
    "trump.hist(column = \"retweet_count\", by = \"conspiracy_talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#distribution of favorites by conspiracy talk\n",
    "trump.hist(column = \"favorite_count\", by = \"conspiracy_talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#distribution of retweets by conspiracy news reference\n",
    "trump.hist(column = \"retweet_count\", by = \"conspiracy_reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#distribution of favorites by conspiracy news reference\n",
    "trump.hist(column = \"favorite_count\", by = \"conspiracy_reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create cleaning function for topic modeling news sites\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "punctuation = set(string.punctuation)\n",
    "lemmatize = WordNetLemmatizer()\n",
    "\n",
    "def cleaning(article):\n",
    "    lower_split = \" \".join([i for i in article.lower().split() if i not in stopwords])\n",
    "    punctuation_removal = \"\".join(i for i in lower_split if i not in punctuation)\n",
    "    lemm = \" \".join(lemmatize.lemmatize(i) for i in punctuation_removal.split())\n",
    "    return lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean conspiracy news for topic modeling\n",
    "conspiracy_news = news[news.conspiracy_talk == True]\n",
    "words = conspiracy_news[\"text\"].apply(cleaning)\n",
    "word_list = [i.split() for i in words]\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build dictionary\n",
    "dictionary = corpora.Dictionary(word_list)\n",
    "dictionary.save(\"dictionary.dict\")\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build corpus - doing manually with gensim more accurate than NLTK\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in word_list]\n",
    "corpora.MmCorpus.serialize(\"corpus.mm\", doc_term_matrix)\n",
    "\n",
    "print(len(doc_term_matrix))\n",
    "print(doc_term_matrix[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a way to monitor the passes by gensim\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO,\n",
    "                   filename='running.log',filemode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LDA for topic model of news\n",
    "import gensim\n",
    "start = time()\n",
    "#LDA model\n",
    "#create the object with gensim\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "#training the LDA model\n",
    "ldamodel = Lda(doc_term_matrix, num_topics = 50, id2word = dictionary, \n",
    "               passes = 5)\n",
    "\n",
    "ldamodel.save(\"topic.model\")\n",
    "print('used: {:.2f}s'.format(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create interactive topic model using the LDAvis import from R\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "d = gensim.corpora.Dictionary.load(\"dictionary.dict\")\n",
    "c = gensim.corpora.MmCorpus(\"corpus.mm\")\n",
    "lda = gensim.models.LdaModel.load(\"topic.model\")\n",
    "\n",
    "data = pyLDAvis.gensim.prepare(lda, c, d)\n",
    "data\n",
    "\n",
    "#ignore the warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create topic model for trump conspiracy talk\n",
    "#clean tweets\n",
    "import re\n",
    "\n",
    "def twitter_clean(x): \n",
    "    x = re.sub(\"[^a-z\\s]\", \" \", x.lower())\n",
    "    x = [w for w in x.split() if w not in set(stopwords)]\n",
    "    return \" \".join(x)\n",
    "\n",
    "#create dataframe\n",
    "trump_conspiracy = trump[trump.conspiracy_talk == True]\n",
    "\n",
    "#clean it\n",
    "trump_conspiracy[\"cleaned_text\"] = trump_conspiracy[\"text\"].apply(twitter_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trump model\n",
    "words = trump_conspiracy[\"cleaned_text\"]\n",
    "word_list = [i.split() for i in words]\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build dictionary\n",
    "dictionary = corpora.Dictionary(word_list)\n",
    "dictionary.save(\"dictionary.dict\")\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build corpus\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in word_list]\n",
    "corpora.MmCorpus.serialize(\"corpus.mm\", doc_term_matrix)\n",
    "\n",
    "print(len(doc_term_matrix))\n",
    "print(doc_term_matrix[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "start = time()\n",
    "\n",
    "#LDA model\n",
    "#create the object with gensim\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "#training the LDA model\n",
    "ldamodel = Lda(doc_term_matrix, num_topics = 50, id2word = dictionary, \n",
    "               passes = 5)\n",
    "\n",
    "ldamodel.save(\"topic.model\")\n",
    "print('used: {:.2f}s'.format(time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the interactive topic model\n",
    "d2 = gensim.corpora.Dictionary.load(\"dictionary.dict\")\n",
    "c2 = gensim.corpora.MmCorpus(\"corpus.mm\")\n",
    "lda2 = gensim.models.LdaModel.load(\"topic.model\")\n",
    "\n",
    "data2 = pyLDAvis.gensim.prepare(lda2, c2, d2)\n",
    "data2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
